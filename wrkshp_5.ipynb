{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_wc0Uj5DAqYVMQCXpc2kwZKJt4LC3mMG",
      "authorship_tag": "ABX9TyMsc9E0ssbw+Da3UT4IAGvV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pritija11/workshop-1/blob/main/wrkshp_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C7eJgwywDVZV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "def cost_function(X, Y, W):\n",
        "\n",
        "  m = len(Y)\n",
        "  cost = np.sum((X.dot(W)-Y) **2)/(2*m)\n",
        "  return cost\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "  cost_history = [0] * iterations\n",
        "  m = len(Y)\n",
        "  for iteration in range(iterations):\n",
        "    Y_pred = X.dot(W)\n",
        "    loss = Y_pred - Y\n",
        "    dw = (X.T.dot(loss))/(m)\n",
        "    W_update = W - alpha * dw\n",
        "    cost = cost_function(X, Y, W_update)\n",
        "    cost_history[iteration] = cost\n",
        "  return W_update, cost_history\n"
      ],
      "metadata": {
        "id": "rYC26W4HG7oy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(Y, Y_pred):\n",
        "\n",
        "  rmse = np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "  return rmse\n"
      ],
      "metadata": {
        "id": "zn46GdAILnsB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def r2(Y, Y_pred):\n",
        "  mean_y = np.mean(Y)\n",
        "  ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "  ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "  r2 = 1 - (ss_res / ss_tot)\n",
        "  return r2\n"
      ],
      "metadata": {
        "id": "WEvVhwrsV5ij"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def main():\n",
        "  data = pd.read_csv(\"/content/drive/MyDrive/dataset/student.csv\")\n",
        "  X = data[['Math', 'Reading']].values\n",
        "  Y = data['Writing'].values\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "  W = np.zeros(X_train.shape[1])\n",
        "  alpha = 0.001\n",
        "  iterations = 1000\n",
        "\n",
        "  W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "  Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "  model_rmse = rmse(Y_test, Y_pred)\n",
        "  model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "  print(\"Final Weights:\", W_optimal)\n",
        "  print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "  print(\"RMSE on Test Set:\", model_rmse)\n",
        "  print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "      main()\n",
        "\n"
      ],
      "metadata": {
        "id": "zV0F0mWxYOiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13cc7dc1-b7e2-4a19-e7ab-bc51ce18c110"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [4.7978325  5.02019875]\n",
            "Cost History (First 10 iterations): [np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246)]\n",
            "RMSE on Test Set: 615.8660062668006\n",
            "R-Squared on Test Set: -1514.236975362205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Did your Model Overfitt, Underfitts, or performance is acceptable.\n",
        "\n",
        "When alpha = 0.00001, The model is underfitting as the RMSE value is very high and R-square is negative. The cost history is slao constant which states Gradient descent did not converge properly."
      ],
      "metadata": {
        "id": "PYtgpDv64u5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Experiment with different value of learning rate, making it higher and lower, observe the result."
      ],
      "metadata": {
        "id": "sZNBPO605Qdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "alpha = 0.0001:\n",
        "Final Weights: [0.47978325 0.50201988]\n",
        "Cost History (First 10 iterations): [np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211), np.float64(17.81379717752211)]\n",
        "RMSE on Test Set: 6.092665443799174\n",
        "R-Squared on Test Set: 0.851706281452247\n",
        "\n",
        "When alpha value is increased from 0.00001 to 0.0001, the model perfomance improved. The RMSE reduced from 63.36 to 6.09, and the R² score increased from −15.04 to 0.85, indicating that the model learned effectively. So, the model perfomance is acceptable neither underfitting nor overfitting."
      ],
      "metadata": {
        "id": "W8a5MMKp6fu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d9d3xtzw5U0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "aplha = 0.001:\n",
        "Final Weights: [4.7978325  5.02019875]\n",
        "Cost History (First 10 iterations): [np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246), np.float64(191077.53315577246)]\n",
        "RMSE on Test Set: 615.8660062668006\n",
        "R-Squared on Test Set: -1514.236975362205\n",
        "\n",
        "The model is unstable and performs poorly.\n",
        "\n"
      ],
      "metadata": {
        "id": "n60v_ecm7kvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0crGMHT58dhX"
      }
    }
  ]
}